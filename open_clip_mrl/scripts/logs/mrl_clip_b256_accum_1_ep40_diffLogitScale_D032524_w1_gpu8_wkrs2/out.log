2024-03-25,17:15:03 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 8.
2024-03-25,17:15:03 | INFO | Loaded ViT-B-16 model config.
2024-03-25,17:15:10 | INFO | Model:
2024-03-25,17:15:10 | INFO | CLIP(
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (logit_scale): ParameterList(
      (0): Parameter containing: [torch.float32 of size  (cuda:0)]
      (1): Parameter containing: [torch.float32 of size  (cuda:0)]
      (2): Parameter containing: [torch.float32 of size  (cuda:0)]
      (3): Parameter containing: [torch.float32 of size  (cuda:0)]
      (4): Parameter containing: [torch.float32 of size  (cuda:0)]
  )
)
2024-03-25,17:15:10 | INFO | Params:
2024-03-25,17:15:10 | INFO |   accum_freq: 1
2024-03-25,17:15:10 | INFO |   aug_cfg: {}
2024-03-25,17:15:10 | INFO |   batch_size: 128
2024-03-25,17:15:10 | INFO |   beta1: 0.9
2024-03-25,17:15:10 | INFO |   beta2: 0.98
2024-03-25,17:15:10 | INFO |   checkpoint_path: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/mrl_clip_b256_accum_1_ep40_diffLogitScale_D032524_w1_gpu8_wkrs2/checkpoints
2024-03-25,17:15:10 | INFO |   coca_caption_loss_weight: 2.0
2024-03-25,17:15:10 | INFO |   coca_contrastive_loss_weight: 1.0
2024-03-25,17:15:10 | INFO |   copy_codebase: False
2024-03-25,17:15:10 | INFO |   csv_caption_key: title
2024-03-25,17:15:10 | INFO |   csv_img_key: filepath
2024-03-25,17:15:10 | INFO |   csv_separator: 	
2024-03-25,17:15:10 | INFO |   dataset_resampled: False
2024-03-25,17:15:10 | INFO |   dataset_type: webdataset
2024-03-25,17:15:10 | INFO |   ddp_static_graph: False
2024-03-25,17:15:10 | INFO |   debug: False
2024-03-25,17:15:10 | INFO |   delete_previous_checkpoint: False
2024-03-25,17:15:10 | INFO |   device: cuda:0
2024-03-25,17:15:10 | INFO |   dist_backend: nccl
2024-03-25,17:15:10 | INFO |   dist_url: env://
2024-03-25,17:15:10 | INFO |   distill: False
2024-03-25,17:15:10 | INFO |   distill_model: None
2024-03-25,17:15:10 | INFO |   distill_pretrained: None
2024-03-25,17:15:10 | INFO |   distributed: True
2024-03-25,17:15:10 | INFO |   epochs: 40
2024-03-25,17:15:10 | INFO |   epochs_cooldown: None
2024-03-25,17:15:10 | INFO |   eps: 1e-06
2024-03-25,17:15:10 | INFO |   force_custom_text: False
2024-03-25,17:15:10 | INFO |   force_image_size: None
2024-03-25,17:15:10 | INFO |   force_mrl_loss: True
2024-03-25,17:15:10 | INFO |   force_patch_dropout: None
2024-03-25,17:15:10 | INFO |   force_quick_gelu: False
2024-03-25,17:15:10 | INFO |   gather_with_grad: True
2024-03-25,17:15:10 | INFO |   grad_checkpointing: False
2024-03-25,17:15:10 | INFO |   grad_clip_norm: None
2024-03-25,17:15:10 | INFO |   horovod: False
2024-03-25,17:15:10 | INFO |   image_mean: None
2024-03-25,17:15:10 | INFO |   image_std: None
2024-03-25,17:15:10 | INFO |   imagenet_v2: None
2024-03-25,17:15:10 | INFO |   imagenet_val: /gscratch/krishna/arnabk1/root/val/
2024-03-25,17:15:10 | INFO |   local_loss: True
2024-03-25,17:15:10 | INFO |   local_rank: 0
2024-03-25,17:15:10 | INFO |   lock_image: False
2024-03-25,17:15:10 | INFO |   lock_image_freeze_bn_stats: False
2024-03-25,17:15:10 | INFO |   lock_image_unlocked_groups: 0
2024-03-25,17:15:10 | INFO |   lock_text: False
2024-03-25,17:15:10 | INFO |   lock_text_freeze_layer_norm: False
2024-03-25,17:15:10 | INFO |   lock_text_unlocked_layers: 0
2024-03-25,17:15:10 | INFO |   log_every_n_steps: 100
2024-03-25,17:15:10 | INFO |   log_level: 20
2024-03-25,17:15:10 | INFO |   log_local: False
2024-03-25,17:15:10 | INFO |   log_path: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/mrl_clip_b256_accum_1_ep40_diffLogitScale_D032524_w1_gpu8_wkrs2/out.log
2024-03-25,17:15:10 | INFO |   logs: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/
2024-03-25,17:15:10 | INFO |   lr: 0.0005
2024-03-25,17:15:10 | INFO |   lr_cooldown_end: 0.0
2024-03-25,17:15:10 | INFO |   lr_cooldown_power: 1.0
2024-03-25,17:15:10 | INFO |   lr_scheduler: cosine
2024-03-25,17:15:10 | INFO |   model: ViT-B-16
2024-03-25,17:15:10 | INFO |   mrl_dim_to_consider: [768, 384, 192, 96, 48]
2024-03-25,17:15:10 | INFO |   mrl_loss_weights: [1.0, 1.0, 1.0, 1.0, 1.0]
2024-03-25,17:15:10 | INFO |   name: mrl_clip_b256_accum_1_ep40_diffLogitScale_D032524_w1_gpu8_wkrs2
2024-03-25,17:15:10 | INFO |   no_set_device_rank: False
2024-03-25,17:15:10 | INFO |   precision: amp
2024-03-25,17:15:10 | INFO |   pretrained: 
2024-03-25,17:15:10 | INFO |   pretrained_image: False
2024-03-25,17:15:10 | INFO |   rank: 0
2024-03-25,17:15:10 | INFO |   remote_sync: None
2024-03-25,17:15:10 | INFO |   remote_sync_frequency: 300
2024-03-25,17:15:10 | INFO |   remote_sync_protocol: s3
2024-03-25,17:15:10 | INFO |   report_to: wandb
2024-03-25,17:15:10 | INFO |   resume: None
2024-03-25,17:15:10 | INFO |   save_frequency: 1
2024-03-25,17:15:10 | INFO |   save_most_recent: False
2024-03-25,17:15:10 | INFO |   seed: 42
2024-03-25,17:15:10 | INFO |   skip_scheduler: False
2024-03-25,17:15:10 | INFO |   tensorboard: False
2024-03-25,17:15:10 | INFO |   tensorboard_path: 
2024-03-25,17:15:10 | INFO |   torchscript: False
2024-03-25,17:15:10 | INFO |   trace: False
2024-03-25,17:15:10 | INFO |   train_data: /gscratch/krishna/chenhaoz/IL/FDT/data/cc3m/{00000..00032}.tar
2024-03-25,17:15:10 | INFO |   train_data_upsampling_factors: None
2024-03-25,17:15:10 | INFO |   train_num_samples: 3308333
2024-03-25,17:15:10 | INFO |   use_bn_sync: False
2024-03-25,17:15:10 | INFO |   val_data: None
2024-03-25,17:15:10 | INFO |   val_frequency: 1
2024-03-25,17:15:10 | INFO |   val_num_samples: None
2024-03-25,17:15:10 | INFO |   wandb: True
2024-03-25,17:15:10 | INFO |   wandb_notes: 
2024-03-25,17:15:10 | INFO |   wandb_project_name: mrl_clip_training
2024-03-25,17:15:10 | INFO |   warmup: 4000
2024-03-25,17:15:10 | INFO |   wd: 0.2
2024-03-25,17:15:10 | INFO |   workers: 2
2024-03-25,17:15:10 | INFO |   world_size: 8
2024-03-25,17:15:10 | INFO |   zeroshot_frequency: 2
2024-03-25,17:15:59 | INFO | Start epoch 0
2024-03-25,17:16:17 | INFO | Train Epoch: 0 [   1024/3309568 (0%)] Data (t): 6.518 Batch (t): 17.281, 59.2573/s, 7.40716/s/gpu LR: 0.000000 Logit Scale: 14.286 ,14.286 ,14.286 ,14.286 ,14.286 Mrl_clip_loss_768: 7.0442 (7.0442) Mrl_clip_loss_384: 7.0468 (7.0468) Mrl_clip_loss_192: 7.0800 (7.0800) Mrl_clip_loss_96: 7.1516 (7.1516) Mrl_clip_loss_48: 7.3168 (7.3168) Loss: 35.639 (35.639)
2024-03-25,17:21:47 | INFO | Train Epoch: 0 [ 103424/3309568 (3%)] Data (t): 1.248 Batch (t): 3.299, 247.680/s, 30.9600/s/gpu LR: 0.000013 Logit Scale: 14.289 ,14.287 ,14.285 ,14.284 ,14.284 Mrl_clip_loss_768: 6.5986 (6.8214) Mrl_clip_loss_384: 6.5970 (6.8219) Mrl_clip_loss_192: 6.6013 (6.8407) Mrl_clip_loss_96: 6.6287 (6.8902) Mrl_clip_loss_48: 6.6292 (6.9730) Loss: 33.055 (34.347)
2024-03-25,17:27:26 | INFO | Train Epoch: 0 [ 205824/3309568 (6%)] Data (t): 1.840 Batch (t): 3.397, 231.181/s, 28.8976/s/gpu LR: 0.000025 Logit Scale: 14.310 ,14.292 ,14.277 ,14.274 ,14.280 Mrl_clip_loss_768: 6.0732 (6.5720) Mrl_clip_loss_384: 6.0598 (6.5678) Mrl_clip_loss_192: 6.0517 (6.5777) Mrl_clip_loss_96: 6.0429 (6.6078) Mrl_clip_loss_48: 6.0501 (6.6654) Loss: 30.278 (32.991)
2024-03-25,17:33:13 | INFO | Train Epoch: 0 [ 308224/3309568 (9%)] Data (t): 1.642 Batch (t): 3.469, 571.654/s, 71.4568/s/gpu LR: 0.000038 Logit Scale: 14.341 ,14.298 ,14.265 ,14.256 ,14.270 Mrl_clip_loss_768: 5.8744 (6.3976) Mrl_clip_loss_384: 5.8590 (6.3906) Mrl_clip_loss_192: 5.8354 (6.3921) Mrl_clip_loss_96: 5.8346 (6.4145) Mrl_clip_loss_48: 5.8760 (6.4680) Loss: 29.279 (32.063)
2024-03-25,17:39:08 | INFO | Train Epoch: 0 [ 410624/3309568 (12%)] Data (t): 1.687 Batch (t): 3.552, 210.555/s, 26.3193/s/gpu LR: 0.000050 Logit Scale: 14.382 ,14.314 ,14.257 ,14.239 ,14.258 Mrl_clip_loss_768: 5.8673 (6.2915) Mrl_clip_loss_384: 5.8609 (6.2847) Mrl_clip_loss_192: 5.8544 (6.2846) Mrl_clip_loss_96: 5.8566 (6.3029) Mrl_clip_loss_48: 5.8623 (6.3469) Loss: 29.302 (31.511)
2024-03-25,17:44:44 | INFO | Train Epoch: 0 [ 513024/3309568 (16%)] Data (t): 1.946 Batch (t): 3.356, 241.828/s, 30.2284/s/gpu LR: 0.000063 Logit Scale: 14.434 ,14.341 ,14.255 ,14.224 ,14.246 Mrl_clip_loss_768: 5.6665 (6.1874) Mrl_clip_loss_384: 5.6757 (6.1832) Mrl_clip_loss_192: 5.6802 (6.1838) Mrl_clip_loss_96: 5.6678 (6.1970) Mrl_clip_loss_48: 5.6844 (6.2365) Loss: 28.375 (30.988)
2024-03-25,17:49:57 | INFO | Train Epoch: 0 [ 615424/3309568 (19%)] Data (t): 0.951 Batch (t): 3.125, 511.271/s, 63.9089/s/gpu LR: 0.000075 Logit Scale: 14.509 ,14.390 ,14.266 ,14.218 ,14.231 Mrl_clip_loss_768: 5.1745 (6.0427) Mrl_clip_loss_384: 5.1723 (6.0388) Mrl_clip_loss_192: 5.1848 (6.0411) Mrl_clip_loss_96: 5.1766 (6.0513) Mrl_clip_loss_48: 5.1794 (6.0855) Loss: 25.888 (30.259)
2024-03-25,17:55:25 | INFO | Train Epoch: 0 [ 717824/3309568 (22%)] Data (t): 1.471 Batch (t): 3.281, 478.208/s, 59.7759/s/gpu LR: 0.000088 Logit Scale: 14.594 ,14.455 ,14.298 ,14.229 ,14.226 Mrl_clip_loss_768: 5.4662 (5.9706) Mrl_clip_loss_384: 5.4543 (5.9657) Mrl_clip_loss_192: 5.4554 (5.9679) Mrl_clip_loss_96: 5.4696 (5.9786) Mrl_clip_loss_48: 5.4707 (6.0086) Loss: 27.316 (29.891)
2024-03-25,18:01:07 | INFO | Train Epoch: 0 [ 820224/3309568 (25%)] Data (t): 0.711 Batch (t): 3.427, 306.088/s, 38.2610/s/gpu LR: 0.000100 Logit Scale: 14.692 ,14.533 ,14.340 ,14.247 ,14.226 Mrl_clip_loss_768: 5.1660 (5.8812) Mrl_clip_loss_384: 5.1674 (5.8770) Mrl_clip_loss_192: 5.1781 (5.8802) Mrl_clip_loss_96: 5.2000 (5.8920) Mrl_clip_loss_48: 5.2071 (5.9195) Loss: 25.919 (29.450)
2024-03-25,18:06:37 | INFO | Train Epoch: 0 [ 922624/3309568 (28%)] Data (t): 0.747 Batch (t): 3.296, 313.619/s, 39.2024/s/gpu LR: 0.000113 Logit Scale: 14.815 ,14.642 ,14.419 ,14.304 ,14.256 Mrl_clip_loss_768: 4.7791 (5.7710) Mrl_clip_loss_384: 4.7661 (5.7659) Mrl_clip_loss_192: 4.7657 (5.7687) Mrl_clip_loss_96: 4.7648 (5.7793) Mrl_clip_loss_48: 4.7824 (5.8058) Loss: 23.858 (28.891)
2024-03-25,18:11:48 | INFO | Train Epoch: 0 [1025024/3309568 (31%)] Data (t): 1.473 Batch (t): 3.106, 242.649/s, 30.3312/s/gpu LR: 0.000125 Logit Scale: 14.945 ,14.761 ,14.513 ,14.376 ,14.306 Mrl_clip_loss_768: 5.0044 (5.7013) Mrl_clip_loss_384: 4.9966 (5.6960) Mrl_clip_loss_192: 4.9923 (5.6981) Mrl_clip_loss_96: 4.9807 (5.7067) Mrl_clip_loss_48: 5.0024 (5.7328) Loss: 24.976 (28.535)
2024-03-25,18:17:14 | INFO | Train Epoch: 0 [1127424/3309568 (34%)] Data (t): 0.512 Batch (t): 3.267, 600.294/s, 75.0367/s/gpu LR: 0.000138 Logit Scale: 15.112 ,14.919 ,14.647 ,14.489 ,14.393 Mrl_clip_loss_768: 4.7291 (5.6203) Mrl_clip_loss_384: 4.7335 (5.6158) Mrl_clip_loss_192: 4.7309 (5.6175) Mrl_clip_loss_96: 4.7546 (5.6274) Mrl_clip_loss_48: 4.7599 (5.6517) Loss: 23.708 (28.133)
2024-03-25,18:22:59 | INFO | Train Epoch: 0 [1229824/3309568 (37%)] Data (t): 1.493 Batch (t): 3.448, 210.789/s, 26.3486/s/gpu LR: 0.000150 Logit Scale: 15.295 ,15.094 ,14.804 ,14.630 ,14.512 Mrl_clip_loss_768: 4.2672 (5.5162) Mrl_clip_loss_384: 4.2645 (5.5118) Mrl_clip_loss_192: 4.2526 (5.5125) Mrl_clip_loss_96: 4.2569 (5.5219) Mrl_clip_loss_48: 4.2646 (5.5450) Loss: 21.306 (27.608)
2024-03-25,18:28:42 | INFO | Train Epoch: 0 [1332224/3309568 (40%)] Data (t): 1.471 Batch (t): 3.433, 313.381/s, 39.1726/s/gpu LR: 0.000163 Logit Scale: 15.501 ,15.293 ,14.988 ,14.796 ,14.665 Mrl_clip_loss_768: 4.4351 (5.4390) Mrl_clip_loss_384: 4.4376 (5.4351) Mrl_clip_loss_192: 4.4488 (5.4365) Mrl_clip_loss_96: 4.4488 (5.4453) Mrl_clip_loss_48: 4.4577 (5.4673) Loss: 22.228 (27.223)
2024-03-25,18:33:57 | INFO | Train Epoch: 0 [1434624/3309568 (43%)] Data (t): 0.469 Batch (t): 3.144, 519.405/s, 64.9257/s/gpu LR: 0.000175 Logit Scale: 15.743 ,15.531 ,15.211 ,15.010 ,14.866 Mrl_clip_loss_768: 4.5565 (5.3802) Mrl_clip_loss_384: 4.5484 (5.3760) Mrl_clip_loss_192: 4.5356 (5.3765) Mrl_clip_loss_96: 4.5354 (5.3846) Mrl_clip_loss_48: 4.5271 (5.4047) Loss: 22.703 (26.922)
2024-03-25,18:38:33 | INFO | Train Epoch: 0 [1537024/3309568 (46%)] Data (t): 0.368 Batch (t): 2.765, 424.138/s, 53.0172/s/gpu LR: 0.000188 Logit Scale: 15.979 ,15.763 ,15.436 ,15.225 ,15.074 Mrl_clip_loss_768: 4.3090 (5.3132) Mrl_clip_loss_384: 4.3103 (5.3094) Mrl_clip_loss_192: 4.3067 (5.3096) Mrl_clip_loss_96: 4.3103 (5.3175) Mrl_clip_loss_48: 4.3207 (5.3369) Loss: 21.557 (26.587)
2024-03-25,18:43:32 | INFO | Train Epoch: 0 [1639424/3309568 (50%)] Data (t): 0.610 Batch (t): 2.985, 451.828/s, 56.4785/s/gpu LR: 0.000200 Logit Scale: 16.296 ,16.075 ,15.737 ,15.516 ,15.362 Mrl_clip_loss_768: 4.3023 (5.2537) Mrl_clip_loss_384: 4.3103 (5.2506) Mrl_clip_loss_192: 4.3232 (5.2516) Mrl_clip_loss_96: 4.3207 (5.2589) Mrl_clip_loss_48: 4.3205 (5.2771) Loss: 21.577 (26.292)
2024-03-25,18:48:34 | INFO | Train Epoch: 0 [1741824/3309568 (53%)] Data (t): 0.380 Batch (t): 3.022, 577.610/s, 72.2012/s/gpu LR: 0.000213 Logit Scale: 16.596 ,16.373 ,16.028 ,15.801 ,15.640 Mrl_clip_loss_768: 4.0093 (5.1846) Mrl_clip_loss_384: 4.0125 (5.1818) Mrl_clip_loss_192: 4.0230 (5.1833) Mrl_clip_loss_96: 4.0346 (5.1908) Mrl_clip_loss_48: 4.0584 (5.2094) Loss: 20.138 (25.950)
2024-03-25,18:54:22 | INFO | Train Epoch: 0 [1844224/3309568 (56%)] Data (t): 1.360 Batch (t): 3.483, 371.040/s, 46.3799/s/gpu LR: 0.000225 Logit Scale: 16.932 ,16.706 ,16.351 ,16.115 ,15.951 Mrl_clip_loss_768: 3.8021 (5.1118) Mrl_clip_loss_384: 3.8018 (5.1092) Mrl_clip_loss_192: 3.8059 (5.1108) Mrl_clip_loss_96: 3.8060 (5.1180) Mrl_clip_loss_48: 3.8329 (5.1370) Loss: 19.049 (25.587)
2024-03-25,19:00:18 | INFO | Train Epoch: 0 [1946624/3309568 (59%)] Data (t): 0.463 Batch (t): 3.556, 220.703/s, 27.5879/s/gpu LR: 0.000238 Logit Scale: 17.327 ,17.095 ,16.732 ,16.488 ,16.319 Mrl_clip_loss_768: 3.3294 (5.0227) Mrl_clip_loss_384: 3.3248 (5.0200) Mrl_clip_loss_192: 3.3251 (5.0216) Mrl_clip_loss_96: 3.3187 (5.0280) Mrl_clip_loss_48: 3.3257 (5.0464) Loss: 16.624 (25.139)
2024-03-25,19:05:45 | INFO | Train Epoch: 0 [2049024/3309568 (62%)] Data (t): 1.886 Batch (t): 3.275, 205.014/s, 25.6267/s/gpu LR: 0.000250 Logit Scale: 17.697 ,17.463 ,17.092 ,16.840 ,16.665 Mrl_clip_loss_768: 3.5659 (4.9533) Mrl_clip_loss_384: 3.5596 (4.9504) Mrl_clip_loss_192: 3.5508 (4.9515) Mrl_clip_loss_96: 3.5511 (4.9577) Mrl_clip_loss_48: 3.5592 (4.9756) Loss: 17.787 (24.789)
2024-03-25,19:11:35 | INFO | Train Epoch: 0 [2151424/3309568 (65%)] Data (t): 1.247 Batch (t): 3.500, 511.481/s, 63.9351/s/gpu LR: 0.000263 Logit Scale: 18.164 ,17.924 ,17.543 ,17.282 ,17.103 Mrl_clip_loss_768: 3.5628 (4.8901) Mrl_clip_loss_384: 3.5624 (4.8873) Mrl_clip_loss_192: 3.5770 (4.8890) Mrl_clip_loss_96: 3.5954 (4.8957) Mrl_clip_loss_48: 3.6104 (4.9135) Loss: 17.908 (24.476)
2024-03-25,19:17:15 | INFO | Train Epoch: 0 [2253824/3309568 (68%)] Data (t): 1.026 Batch (t): 3.398, 317.527/s, 39.6909/s/gpu LR: 0.000275 Logit Scale: 18.610 ,18.365 ,17.976 ,17.707 ,17.521 Mrl_clip_loss_768: 3.4909 (4.8293) Mrl_clip_loss_384: 3.4919 (4.8267) Mrl_clip_loss_192: 3.4984 (4.8286) Mrl_clip_loss_96: 3.5104 (4.8355) Mrl_clip_loss_48: 3.5427 (4.8539) Loss: 17.534 (24.174)
2024-03-25,19:22:49 | INFO | Train Epoch: 0 [2356224/3309568 (71%)] Data (t): 0.900 Batch (t): 3.344, 262.620/s, 32.8275/s/gpu LR: 0.000288 Logit Scale: 19.055 ,18.808 ,18.408 ,18.130 ,17.937 Mrl_clip_loss_768: 3.2961 (4.7654) Mrl_clip_loss_384: 3.2949 (4.7628) Mrl_clip_loss_192: 3.2917 (4.7645) Mrl_clip_loss_96: 3.3088 (4.7719) Mrl_clip_loss_48: 3.3237 (4.7902) Loss: 16.515 (23.855)
2024-03-25,19:28:47 | INFO | Train Epoch: 0 [2458624/3309568 (74%)] Data (t): 1.035 Batch (t): 3.570, 429.058/s, 53.6323/s/gpu LR: 0.000300 Logit Scale: 19.641 ,19.387 ,18.978 ,18.692 ,18.493 Mrl_clip_loss_768: 3.3731 (4.7097) Mrl_clip_loss_384: 3.3713 (4.7072) Mrl_clip_loss_192: 3.3843 (4.7093) Mrl_clip_loss_96: 3.3951 (4.7168) Mrl_clip_loss_48: 3.4392 (4.7361) Loss: 16.963 (23.579)
2024-03-25,19:33:55 | INFO | Train Epoch: 0 [2561024/3309568 (77%)] Data (t): 1.075 Batch (t): 3.085, 266.484/s, 33.3106/s/gpu LR: 0.000313 Logit Scale: 20.149 ,19.890 ,19.472 ,19.175 ,18.968 Mrl_clip_loss_768: 3.0622 (4.6464) Mrl_clip_loss_384: 3.0632 (4.6440) Mrl_clip_loss_192: 3.0684 (4.6462) Mrl_clip_loss_96: 3.0811 (4.6539) Mrl_clip_loss_48: 3.0850 (4.6726) Loss: 15.360 (23.263)
2024-03-25,19:38:47 | INFO | Train Epoch: 0 [2663424/3309568 (80%)] Data (t): 0.996 Batch (t): 2.920, 358.825/s, 44.8532/s/gpu LR: 0.000325 Logit Scale: 20.773 ,20.509 ,20.076 ,19.769 ,19.553 Mrl_clip_loss_768: 3.3012 (4.5965) Mrl_clip_loss_384: 3.2967 (4.5941) Mrl_clip_loss_192: 3.2924 (4.5961) Mrl_clip_loss_96: 3.3090 (4.6041) Mrl_clip_loss_48: 3.3177 (4.6224) Loss: 16.517 (23.013)
2024-03-25,19:43:39 | INFO | Train Epoch: 0 [2765824/3309568 (84%)] Data (t): 1.108 Batch (t): 2.916, 240.594/s, 30.0743/s/gpu LR: 0.000338 Logit Scale: 21.380 ,21.114 ,20.670 ,20.353 ,20.129 Mrl_clip_loss_768: 2.8103 (4.5327) Mrl_clip_loss_384: 2.8077 (4.5303) Mrl_clip_loss_192: 2.8187 (4.5326) Mrl_clip_loss_96: 2.8244 (4.5405) Mrl_clip_loss_48: 2.7987 (4.5573) Loss: 14.060 (22.693)
2024-03-25,19:48:12 | INFO | Train Epoch: 0 [2868224/3309568 (87%)] Data (t): 1.235 Batch (t): 2.733, 349.579/s, 43.6974/s/gpu LR: 0.000350 Logit Scale: 21.929 ,21.664 ,21.211 ,20.880 ,20.642 Mrl_clip_loss_768: 2.6710 (4.4685) Mrl_clip_loss_384: 2.6749 (4.4663) Mrl_clip_loss_192: 2.6530 (4.4678) Mrl_clip_loss_96: 2.6695 (4.4760) Mrl_clip_loss_48: 2.6995 (4.4932) Loss: 13.368 (22.372)
2024-03-25,19:53:03 | INFO | Train Epoch: 0 [2970624/3309568 (90%)] Data (t): 0.550 Batch (t): 2.909, 434.960/s, 54.3700/s/gpu LR: 0.000363 Logit Scale: 22.736 ,22.463 ,21.993 ,21.650 ,21.401 Mrl_clip_loss_768: 2.7793 (4.4122) Mrl_clip_loss_384: 2.7692 (4.4097) Mrl_clip_loss_192: 2.7655 (4.4110) Mrl_clip_loss_96: 2.7775 (4.4194) Mrl_clip_loss_48: 2.7745 (4.4360) Loss: 13.866 (22.088)
2024-03-25,19:58:03 | INFO | Train Epoch: 0 [3073024/3309568 (93%)] Data (t): 0.582 Batch (t): 3.000, 497.689/s, 62.2111/s/gpu LR: 0.000375 Logit Scale: 23.329 ,23.058 ,22.580 ,22.227 ,21.960 Mrl_clip_loss_768: 2.8091 (4.3605) Mrl_clip_loss_384: 2.8068 (4.3580) Mrl_clip_loss_192: 2.7878 (4.3587) Mrl_clip_loss_96: 2.7912 (4.3669) Mrl_clip_loss_48: 2.7813 (4.3826) Loss: 13.976 (21.827)
2024-03-25,20:02:53 | INFO | Train Epoch: 0 [3175424/3309568 (96%)] Data (t): 1.120 Batch (t): 2.902, 244.102/s, 30.5128/s/gpu LR: 0.000388 Logit Scale: 24.046 ,23.777 ,23.294 ,22.921 ,22.631 Mrl_clip_loss_768: 2.5964 (4.3054) Mrl_clip_loss_384: 2.5842 (4.3026) Mrl_clip_loss_192: 2.5864 (4.3033) Mrl_clip_loss_96: 2.5926 (4.3114) Mrl_clip_loss_48: 2.6444 (4.3283) Loss: 13.004 (21.551)
2024-03-25,20:07:34 | INFO | Train Epoch: 0 [3277824/3309568 (99%)] Data (t): 1.255 Batch (t): 2.806, 304.398/s, 38.0497/s/gpu LR: 0.000400 Logit Scale: 24.780 ,24.513 ,24.024 ,23.635 ,23.330 Mrl_clip_loss_768: 2.5485 (4.2522) Mrl_clip_loss_384: 2.5423 (4.2492) Mrl_clip_loss_192: 2.5132 (4.2491) Mrl_clip_loss_96: 2.5243 (4.2573) Mrl_clip_loss_48: 2.5401 (4.2741) Loss: 12.668 (21.282)
2024-03-25,20:08:54 | INFO | Train Epoch: 0 [3309568/3309568 (100%)] Data (t): 1.337 Batch (t): 2.586, 1219.72/s, 152.466/s/gpu LR: 0.000404 Logit Scale: 24.986 ,24.719 ,24.229 ,23.839 ,23.520 Mrl_clip_loss_768: 2.2539 (4.1934) Mrl_clip_loss_384: 2.2490 (4.1904) Mrl_clip_loss_192: 2.2558 (4.1904) Mrl_clip_loss_96: 2.2741 (4.1990) Mrl_clip_loss_48: 2.2953 (4.2159) Loss: 11.328 (20.989)
2024-03-25,20:09:07 | INFO | Start epoch 1
2024-03-25,20:09:12 | INFO | Train Epoch: 1 [   1024/3309568 (0%)] Data (t): 4.064 Batch (t): 4.766, 214.839/s, 26.8548/s/gpu LR: 0.000404 Logit Scale: 24.992 ,24.725 ,24.234 ,23.845 ,23.526 Mrl_clip_loss_768: 1.7161 (1.7161) Mrl_clip_loss_384: 1.7080 (1.7080) Mrl_clip_loss_192: 1.7011 (1.7011) Mrl_clip_loss_96: 1.6937 (1.6937) Mrl_clip_loss_48: 1.6953 (1.6953) Loss: 8.5142 (8.5142)
2024-03-25,20:14:45 | INFO | Train Epoch: 1 [ 103424/3309568 (3%)] Data (t): 1.769 Batch (t): 3.328, 288.194/s, 36.0242/s/gpu LR: 0.000417 Logit Scale: 25.845 ,25.572 ,25.070 ,24.672 ,24.327 Mrl_clip_loss_768: 2.0224 (1.8692) Mrl_clip_loss_384: 2.0120 (1.8600) Mrl_clip_loss_192: 1.9847 (1.8429) Mrl_clip_loss_96: 1.9722 (1.8329) Mrl_clip_loss_48: 2.0129 (1.8541) Loss: 10.004 (9.2591)
2024-03-25,20:20:34 | INFO | Train Epoch: 1 [ 205824/3309568 (6%)] Data (t): 1.599 Batch (t): 3.488, 208.769/s, 26.0961/s/gpu LR: 0.000429 Logit Scale: 26.107 ,25.845 ,25.355 ,24.927 ,24.547 Mrl_clip_loss_768: 2.0961 (1.9449) Mrl_clip_loss_384: 2.0899 (1.9366) Mrl_clip_loss_192: 2.0786 (1.9215) Mrl_clip_loss_96: 2.0729 (1.9129) Mrl_clip_loss_48: 2.0911 (1.9331) Loss: 10.429 (9.6490)
2024-03-25,20:26:23 | INFO | Train Epoch: 1 [ 308224/3309568 (9%)] Data (t): 1.739 Batch (t): 3.490, 510.740/s, 63.8425/s/gpu LR: 0.000442 Logit Scale: 26.689 ,26.420 ,25.923 ,25.463 ,25.054 Mrl_clip_loss_768: 2.3886 (2.0558) Mrl_clip_loss_384: 2.3880 (2.0495) Mrl_clip_loss_192: 2.3707 (2.0338) Mrl_clip_loss_96: 2.3617 (2.0251) Mrl_clip_loss_48: 2.3455 (2.0362) Loss: 11.854 (10.200)
2024-03-25,20:32:20 | INFO | Train Epoch: 1 [ 410624/3309568 (12%)] Data (t): 2.076 Batch (t): 3.567, 211.703/s, 26.4629/s/gpu LR: 0.000454 Logit Scale: 27.397 ,27.135 ,26.640 ,26.182 ,25.754 Mrl_clip_loss_768: 2.0053 (2.0457) Mrl_clip_loss_384: 2.0084 (2.0413) Mrl_clip_loss_192: 2.0155 (2.0301) Mrl_clip_loss_96: 2.0104 (2.0222) Mrl_clip_loss_48: 2.0006 (2.0291) Loss: 10.040 (10.168)
2024-03-25,20:37:57 | INFO | Train Epoch: 1 [ 513024/3309568 (16%)] Data (t): 1.960 Batch (t): 3.377, 229.260/s, 28.6575/s/gpu LR: 0.000467 Logit Scale: 27.729 ,27.479 ,27.008 ,26.513 ,26.050 Mrl_clip_loss_768: 2.2970 (2.0876) Mrl_clip_loss_384: 2.3013 (2.0846) Mrl_clip_loss_192: 2.2823 (2.0721) Mrl_clip_loss_96: 2.2597 (2.0618) Mrl_clip_loss_48: 2.2701 (2.0692) Loss: 11.410 (10.375)
2024-03-25,20:43:58 | INFO | Train Epoch: 1 [ 615424/3309568 (19%)] Data (t): 1.551 Batch (t): 3.606, 546.088/s, 68.2610/s/gpu LR: 0.000479 Logit Scale: 28.667 ,28.422 ,27.948 ,27.438 ,26.959 Mrl_clip_loss_768: 1.8856 (2.0587) Mrl_clip_loss_384: 1.8828 (2.0558) Mrl_clip_loss_192: 1.8750 (2.0440) Mrl_clip_loss_96: 1.8692 (2.0343) Mrl_clip_loss_48: 1.8636 (2.0399) Loss: 9.3763 (10.233)
2024-03-25,20:49:39 | INFO | Train Epoch: 1 [ 717824/3309568 (22%)] Data (t): 1.446 Batch (t): 3.412, 524.627/s, 65.5783/s/gpu LR: 0.000492 Logit Scale: 28.991 ,28.761 ,28.290 ,27.797 ,27.305 Mrl_clip_loss_768: 2.1317 (2.0678) Mrl_clip_loss_384: 2.1284 (2.0649) Mrl_clip_loss_192: 2.1265 (2.0543) Mrl_clip_loss_96: 2.1149 (2.0443) Mrl_clip_loss_48: 2.1441 (2.0529) Loss: 10.646 (10.284)
2024-03-25,20:55:26 | INFO | Train Epoch: 1 [ 820224/3309568 (25%)] Data (t): 0.716 Batch (t): 3.468, 518.504/s, 64.8129/s/gpu LR: 0.000500 Logit Scale: 29.532 ,29.301 ,28.825 ,28.308 ,27.777 Mrl_clip_loss_768: 2.2865 (2.0921) Mrl_clip_loss_384: 2.2866 (2.0895) Mrl_clip_loss_192: 2.2910 (2.0806) Mrl_clip_loss_96: 2.2982 (2.0725) Mrl_clip_loss_48: 2.3065 (2.0811) Loss: 11.469 (10.416)
2024-03-25,21:00:50 | INFO | Train Epoch: 1 [ 922624/3309568 (28%)] Data (t): 0.812 Batch (t): 3.239, 406.969/s, 50.8712/s/gpu LR: 0.000500 Logit Scale: 30.235 ,30.005 ,29.545 ,29.068 ,28.510 Mrl_clip_loss_768: 2.2676 (2.1097) Mrl_clip_loss_384: 2.2656 (2.1071) Mrl_clip_loss_192: 2.2587 (2.0984) Mrl_clip_loss_96: 2.2513 (2.0904) Mrl_clip_loss_48: 2.2721 (2.1002) Loss: 11.315 (10.506)
2024-03-25,21:08:11 | INFO | Train Epoch: 1 [1025024/3309568 (31%)] Data (t): 2.445 Batch (t): 4.411, 160.469/s, 20.0586/s/gpu LR: 0.000500 Logit Scale: 30.319 ,30.100 ,29.644 ,29.143 ,28.577 Mrl_clip_loss_768: 2.2721 (2.1245) Mrl_clip_loss_384: 2.2705 (2.1220) Mrl_clip_loss_192: 2.2614 (2.1132) Mrl_clip_loss_96: 2.2540 (2.1053) Mrl_clip_loss_48: 2.3157 (2.1198) Loss: 11.374 (10.585)
2024-03-25,21:17:08 | INFO | Train Epoch: 1 [1127424/3309568 (34%)] Data (t): 2.073 Batch (t): 5.374, 222.990/s, 27.8738/s/gpu LR: 0.000500 Logit Scale: 31.267 ,31.056 ,30.611 ,30.097 ,29.538 Mrl_clip_loss_768: 1.9022 (2.1059) Mrl_clip_loss_384: 1.8940 (2.1030) Mrl_clip_loss_192: 1.8885 (2.0945) Mrl_clip_loss_96: 1.9093 (2.0890) Mrl_clip_loss_48: 1.8765 (2.0995) Loss: 9.4706 (10.492)
2024-03-25,21:23:52 | INFO | Train Epoch: 1 [1229824/3309568 (37%)] Data (t): 2.219 Batch (t): 4.035, 247.021/s, 30.8777/s/gpu LR: 0.000500 Logit Scale: 31.704 ,31.488 ,31.064 ,30.566 ,29.960 Mrl_clip_loss_768: 1.8639 (2.0873) Mrl_clip_loss_384: 1.8641 (2.0846) Mrl_clip_loss_192: 1.8548 (2.0761) Mrl_clip_loss_96: 1.8647 (2.0717) Mrl_clip_loss_48: 1.8854 (2.0830) Loss: 9.3329 (10.403)
2024-03-25,21:28:27 | INFO | Train Epoch: 1 [1332224/3309568 (40%)] Data (t): 0.974 Batch (t): 2.750, 379.876/s, 47.4845/s/gpu LR: 0.000500 Logit Scale: 32.220 ,32.013 ,31.620 ,31.110 ,30.464 Mrl_clip_loss_768: 1.6517 (2.0562) Mrl_clip_loss_384: 1.6472 (2.0533) Mrl_clip_loss_192: 1.6545 (2.0460) Mrl_clip_loss_96: 1.6520 (2.0417) Mrl_clip_loss_48: 1.6449 (2.0517) Loss: 8.2503 (10.249)
2024-03-25,21:33:17 | INFO | Train Epoch: 1 [1434624/3309568 (43%)] Data (t): 0.571 Batch (t): 2.898, 633.116/s, 79.1395/s/gpu LR: 0.000500 Logit Scale: 33.105 ,32.910 ,32.500 ,32.000 ,31.319 Mrl_clip_loss_768: 1.7776 (2.0376) Mrl_clip_loss_384: 1.7651 (2.0341) Mrl_clip_loss_192: 1.7620 (2.0270) Mrl_clip_loss_96: 1.7585 (2.0228) Mrl_clip_loss_48: 1.7321 (2.0304) Loss: 8.7954 (10.152)
2024-03-25,21:37:40 | INFO | Train Epoch: 1 [1537024/3309568 (46%)] Data (t): 1.017 Batch (t): 2.636, 568.303/s, 71.0378/s/gpu LR: 0.000500 Logit Scale: 33.315 ,33.115 ,32.702 ,32.221 ,31.517 Mrl_clip_loss_768: 1.9404 (2.0316) Mrl_clip_loss_384: 1.9374 (2.0281) Mrl_clip_loss_192: 1.9442 (2.0219) Mrl_clip_loss_96: 1.9219 (2.0165) Mrl_clip_loss_48: 1.9689 (2.0266) Loss: 9.7128 (10.125)
2024-03-25,21:43:00 | INFO | Train Epoch: 1 [1639424/3309568 (50%)] Data (t): 1.072 Batch (t): 3.204, 440.132/s, 55.0165/s/gpu LR: 0.000500 Logit Scale: 34.213 ,34.015 ,33.609 ,33.095 ,32.381 Mrl_clip_loss_768: 1.4449 (1.9970) Mrl_clip_loss_384: 1.4399 (1.9935) Mrl_clip_loss_192: 1.4378 (1.9875) Mrl_clip_loss_96: 1.4330 (1.9822) Mrl_clip_loss_48: 1.4249 (1.9912) Loss: 7.1806 (9.9514)
2024-03-25,21:49:16 | INFO | Train Epoch: 1 [1741824/3309568 (53%)] Data (t): 0.949 Batch (t): 3.751, 580.877/s, 72.6096/s/gpu LR: 0.000500 Logit Scale: 34.779 ,34.581 ,34.216 ,33.720 ,32.993 Mrl_clip_loss_768: 2.0065 (1.9976) Mrl_clip_loss_384: 1.9991 (1.9938) Mrl_clip_loss_192: 1.9965 (1.9880) Mrl_clip_loss_96: 2.0006 (1.9832) Mrl_clip_loss_48: 2.0086 (1.9922) Loss: 10.011 (9.9548)
2024-03-25,21:55:21 | INFO | Train Epoch: 1 [1844224/3309568 (56%)] Data (t): 1.167 Batch (t): 3.653, 281.479/s, 35.1848/s/gpu LR: 0.000500 Logit Scale: 34.995 ,34.806 ,34.446 ,33.943 ,33.160 Mrl_clip_loss_768: 1.5094 (1.9719) Mrl_clip_loss_384: 1.5070 (1.9682) Mrl_clip_loss_192: 1.4859 (1.9616) Mrl_clip_loss_96: 1.4758 (1.9565) Mrl_clip_loss_48: 1.4773 (1.9651) Loss: 7.4554 (9.8232)
2024-03-25,22:01:14 | INFO | Train Epoch: 1 [1946624/3309568 (59%)] Data (t): 0.685 Batch (t): 3.532, 562.613/s, 70.3267/s/gpu LR: 0.000500 Logit Scale: 35.850 ,35.653 ,35.281 ,34.777 ,33.992 Mrl_clip_loss_768: 1.2481 (1.9357) Mrl_clip_loss_384: 1.2466 (1.9321) Mrl_clip_loss_192: 1.2478 (1.9259) Mrl_clip_loss_96: 1.2377 (1.9206) Mrl_clip_loss_48: 1.2440 (1.9290) Loss: 6.2242 (9.6433)
2024-03-25,22:05:48 | INFO | Train Epoch: 1 [2049024/3309568 (62%)] Data (t): 1.054 Batch (t): 2.736, 364.393/s, 45.5491/s/gpu LR: 0.000500 Logit Scale: 36.057 ,35.852 ,35.520 ,35.003 ,34.170 Mrl_clip_loss_768: 1.5996 (1.9197) Mrl_clip_loss_384: 1.5965 (1.9161) Mrl_clip_loss_192: 1.5982 (1.9103) Mrl_clip_loss_96: 1.5755 (1.9042) Mrl_clip_loss_48: 1.6066 (1.9137) Loss: 7.9764 (9.5639)
2024-03-25,22:10:37 | INFO | Train Epoch: 1 [2151424/3309568 (65%)] Data (t): 0.620 Batch (t): 2.888, 524.754/s, 65.5942/s/gpu LR: 0.000500 Logit Scale: 36.765 ,36.554 ,36.210 ,35.689 ,34.827 Mrl_clip_loss_768: 1.2994 (1.8915) Mrl_clip_loss_384: 1.3075 (1.8884) Mrl_clip_loss_192: 1.2993 (1.8825) Mrl_clip_loss_96: 1.3088 (1.8771) Mrl_clip_loss_48: 1.3448 (1.8878) Loss: 6.5598 (9.4273)
2024-03-25,22:15:23 | INFO | Train Epoch: 1 [2253824/3309568 (68%)] Data (t): 0.939 Batch (t): 2.866, 519.946/s, 64.9933/s/gpu LR: 0.000500 Logit Scale: 37.145 ,36.934 ,36.617 ,36.100 ,35.216 Mrl_clip_loss_768: 1.7131 (1.8837) Mrl_clip_loss_384: 1.7022 (1.8804) Mrl_clip_loss_192: 1.6936 (1.8743) Mrl_clip_loss_96: 1.6852 (1.8687) Mrl_clip_loss_48: 1.7228 (1.8806) Loss: 8.5170 (9.3878)
2024-03-25,22:19:53 | INFO | Train Epoch: 1 [2356224/3309568 (71%)] Data (t): 0.441 Batch (t): 2.699, 348.874/s, 43.6093/s/gpu LR: 0.000500 Logit Scale: 37.266 ,37.053 ,36.707 ,36.183 ,35.277 Mrl_clip_loss_768: 1.0610 (1.8495) Mrl_clip_loss_384: 1.0548 (1.8460) Mrl_clip_loss_192: 1.0448 (1.8397) Mrl_clip_loss_96: 1.0607 (1.8351) Mrl_clip_loss_48: 1.0603 (1.8464) Loss: 5.2817 (9.2167)
2024-03-25,22:24:52 | INFO | Train Epoch: 1 [2458624/3309568 (74%)] Data (t): 0.787 Batch (t): 2.985, 543.212/s, 67.9015/s/gpu LR: 0.000500 Logit Scale: 38.254 ,38.034 ,37.704 ,37.199 ,36.252 Mrl_clip_loss_768: 1.2232 (1.8244) Mrl_clip_loss_384: 1.2127 (1.8206) Mrl_clip_loss_192: 1.1907 (1.8138) Mrl_clip_loss_96: 1.2063 (1.8099) Mrl_clip_loss_48: 1.2573 (1.8229) Loss: 6.0902 (9.0916)
2024-03-25,22:29:22 | INFO | Train Epoch: 1 [2561024/3309568 (77%)] Data (t): 0.708 Batch (t): 2.700, 392.941/s, 49.1176/s/gpu LR: 0.000500 Logit Scale: 38.195 ,37.972 ,37.694 ,37.159 ,36.129 Mrl_clip_loss_768: 1.3507 (1.8062) Mrl_clip_loss_384: 1.3526 (1.8026) Mrl_clip_loss_192: 1.3405 (1.7956) Mrl_clip_loss_96: 1.3438 (1.7920) Mrl_clip_loss_48: 1.3523 (1.8048) Loss: 6.7398 (9.0012)
2024-03-25,22:34:09 | INFO | Train Epoch: 1 [2663424/3309568 (80%)] Data (t): 0.548 Batch (t): 2.872, 633.500/s, 79.1876/s/gpu LR: 0.000500 Logit Scale: 38.841 ,38.605 ,38.318 ,37.781 ,36.755 Mrl_clip_loss_768: 1.3685 (1.7900) Mrl_clip_loss_384: 1.3589 (1.7862) Mrl_clip_loss_192: 1.3584 (1.7794) Mrl_clip_loss_96: 1.3633 (1.7761) Mrl_clip_loss_48: 1.3799 (1.7890) Loss: 6.8290 (8.9207)
2024-03-25,22:38:51 | INFO | Train Epoch: 1 [2765824/3309568 (84%)] Data (t): 0.794 Batch (t): 2.826, 236.828/s, 29.6035/s/gpu LR: 0.000500 Logit Scale: 39.182 ,38.945 ,38.684 ,38.187 ,37.147 Mrl_clip_loss_768: 0.84279 (1.7562) Mrl_clip_loss_384: 0.83837 (1.7523) Mrl_clip_loss_192: 0.83709 (1.7457) Mrl_clip_loss_96: 0.82947 (1.7423) Mrl_clip_loss_48: 0.85815 (1.7558) Loss: 4.2059 (8.7523)
2024-03-25,22:43:23 | INFO | Train Epoch: 1 [2868224/3309568 (87%)] Data (t): 1.120 Batch (t): 2.717, 385.331/s, 48.1664/s/gpu LR: 0.000500 Logit Scale: 39.020 ,38.791 ,38.549 ,38.022 ,36.955 Mrl_clip_loss_768: 1.0488 (1.7318) Mrl_clip_loss_384: 1.0488 (1.7281) Mrl_clip_loss_192: 1.0429 (1.7215) Mrl_clip_loss_96: 1.0405 (1.7181) Mrl_clip_loss_48: 1.0620 (1.7319) Loss: 5.2430 (8.6313)
2024-03-25,22:48:11 | INFO | Train Epoch: 1 [2970624/3309568 (90%)] Data (t): 0.744 Batch (t): 2.882, 632.185/s, 79.0231/s/gpu LR: 0.000500 Logit Scale: 40.051 ,39.816 ,39.581 ,39.048 ,37.930 Mrl_clip_loss_768: 1.0603 (1.7094) Mrl_clip_loss_384: 1.0538 (1.7056) Mrl_clip_loss_192: 1.0592 (1.6994) Mrl_clip_loss_96: 1.0503 (1.6958) Mrl_clip_loss_48: 1.1007 (1.7108) Loss: 5.3244 (8.5211)
2024-03-25,22:52:40 | INFO | Train Epoch: 1 [3073024/3309568 (93%)] Data (t): 1.055 Batch (t): 2.685, 331.979/s, 41.4973/s/gpu LR: 0.000500 Logit Scale: 39.882 ,39.642 ,39.416 ,38.900 ,37.764 Mrl_clip_loss_768: 1.1335 (1.6908) Mrl_clip_loss_384: 1.1307 (1.6871) Mrl_clip_loss_192: 1.1346 (1.6812) Mrl_clip_loss_96: 1.1279 (1.6775) Mrl_clip_loss_48: 1.1704 (1.6934) Loss: 5.6970 (8.4300)
2024-03-25,22:57:21 | INFO | Train Epoch: 1 [3175424/3309568 (96%)] Data (t): 0.609 Batch (t): 2.817, 588.852/s, 73.6064/s/gpu LR: 0.000500 Logit Scale: 40.470 ,40.223 ,40.003 ,39.465 ,38.304 Mrl_clip_loss_768: 1.0284 (1.6701) Mrl_clip_loss_384: 1.0256 (1.6664) Mrl_clip_loss_192: 1.0203 (1.6605) Mrl_clip_loss_96: 1.0098 (1.6567) Mrl_clip_loss_48: 1.0286 (1.6726) Loss: 5.1126 (8.3263)
2024-03-25,23:02:01 | INFO | Train Epoch: 1 [3277824/3309568 (99%)] Data (t): 1.103 Batch (t): 2.797, 479.318/s, 59.9148/s/gpu LR: 0.000500 Logit Scale: 40.736 ,40.482 ,40.271 ,39.762 ,38.577 Mrl_clip_loss_768: 0.92248 (1.6474) Mrl_clip_loss_384: 0.92300 (1.6439) Mrl_clip_loss_192: 0.92197 (1.6382) Mrl_clip_loss_96: 0.91880 (1.6343) Mrl_clip_loss_48: 0.93587 (1.6503) Loss: 4.6221 (8.2141)
2024-03-25,23:03:20 | INFO | Train Epoch: 1 [3309568/3309568 (100%)] Data (t): 1.307 Batch (t): 2.560, 1103.64/s, 137.955/s/gpu LR: 0.000500 Logit Scale: 40.754 ,40.495 ,40.306 ,39.798 ,38.597 Mrl_clip_loss_768: 0.73209 (1.6205) Mrl_clip_loss_384: 0.72425 (1.6168) Mrl_clip_loss_192: 0.71852 (1.6111) Mrl_clip_loss_96: 0.74362 (1.6081) Mrl_clip_loss_48: 0.75799 (1.6241) Loss: 3.6765 (8.0806)
2024-03-25,23:03:20 | INFO | Starting zero-shot imagenet.
2024-03-25,23:03:20 | INFO | Building zero-shot classifier dim-768
2024-03-25,23:04:28 | INFO | Using classifier
