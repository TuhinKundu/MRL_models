[2024-03-27 13:38:29,179] torch.distributed.run: [WARNING] 
[2024-03-27 13:38:29,179] torch.distributed.run: [WARNING] *****************************************
[2024-03-27 13:38:29,179] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-27 13:38:29,179] torch.distributed.run: [WARNING] *****************************************
2024-03-27,13:44:44 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 4.
2024-03-27,13:44:44 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 4.
2024-03-27,13:44:44 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 4.
2024-03-27,13:44:44 | INFO | Loaded ViT-B-16 model config.
2024-03-27,13:44:44 | INFO | Loaded ViT-B-16 model config.
2024-03-27,13:44:44 | INFO | Loaded ViT-B-16 model config.
2024-03-27,13:44:44 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 4.
2024-03-27,13:44:44 | INFO | Loaded ViT-B-16 model config.
2024-03-27,13:44:50 | INFO | Model:
2024-03-27,13:44:50 | INFO | CLIP(
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (logit_scale): ParameterList(
      (0): Parameter containing: [torch.float32 of size  (cuda:0)]
      (1): Parameter containing: [torch.float32 of size  (cuda:0)]
      (2): Parameter containing: [torch.float32 of size  (cuda:0)]
      (3): Parameter containing: [torch.float32 of size  (cuda:0)]
      (4): Parameter containing: [torch.float32 of size  (cuda:0)]
  )
)
2024-03-27,13:44:50 | INFO | Params:
2024-03-27,13:44:51 | INFO |   accum_freq: 32
2024-03-27,13:44:51 | INFO |   aug_cfg: {}
2024-03-27,13:44:51 | INFO |   batch_size: 256
2024-03-27,13:44:51 | INFO |   beta1: 0.9
2024-03-27,13:44:51 | INFO |   beta2: 0.98
2024-03-27,13:44:51 | INFO |   checkpoint_path: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4/checkpoints
2024-03-27,13:44:51 | INFO |   coca_caption_loss_weight: 2.0
2024-03-27,13:44:51 | INFO |   coca_contrastive_loss_weight: 1.0
2024-03-27,13:44:51 | INFO |   copy_codebase: False
2024-03-27,13:44:51 | INFO |   csv_caption_key: title
2024-03-27,13:44:51 | INFO |   csv_img_key: filepath
2024-03-27,13:44:51 | INFO |   csv_separator: 	
2024-03-27,13:44:51 | INFO |   dataset_resampled: False
2024-03-27,13:44:51 | INFO |   dataset_type: webdataset
2024-03-27,13:44:51 | INFO |   ddp_static_graph: False
2024-03-27,13:44:51 | INFO |   debug: False
2024-03-27,13:44:51 | INFO |   delete_previous_checkpoint: False
2024-03-27,13:44:51 | INFO |   device: cuda:0
2024-03-27,13:44:51 | INFO |   dist_backend: nccl
2024-03-27,13:44:51 | INFO |   dist_url: env://
2024-03-27,13:44:51 | INFO |   distill: False
2024-03-27,13:44:51 | INFO |   distill_model: None
2024-03-27,13:44:51 | INFO |   distill_pretrained: None
2024-03-27,13:44:51 | INFO |   distributed: True
2024-03-27,13:44:51 | INFO |   epochs: 40
2024-03-27,13:44:51 | INFO |   epochs_cooldown: None
2024-03-27,13:44:51 | INFO |   eps: 1e-06
2024-03-27,13:44:51 | INFO |   force_custom_text: False
2024-03-27,13:44:51 | INFO |   force_image_size: None
2024-03-27,13:44:51 | INFO |   force_mrl_loss: True
2024-03-27,13:44:51 | INFO |   force_patch_dropout: None
2024-03-27,13:44:51 | INFO |   force_quick_gelu: False
2024-03-27,13:44:51 | INFO |   gather_with_grad: True
2024-03-27,13:44:51 | INFO |   grad_checkpointing: False
2024-03-27,13:44:51 | INFO |   grad_clip_norm: None
2024-03-27,13:44:51 | INFO |   horovod: False
2024-03-27,13:44:51 | INFO |   image_mean: None
2024-03-27,13:44:51 | INFO |   image_std: None
2024-03-27,13:44:51 | INFO |   imagenet_v2: None
2024-03-27,13:44:51 | INFO |   imagenet_val: /gscratch/krishna/arnabk1/root/val/
2024-03-27,13:44:51 | INFO |   local_loss: True
2024-03-27,13:44:51 | INFO |   local_rank: 0
2024-03-27,13:44:51 | INFO |   lock_image: False
2024-03-27,13:44:51 | INFO |   lock_image_freeze_bn_stats: False
2024-03-27,13:44:51 | INFO |   lock_image_unlocked_groups: 0
2024-03-27,13:44:51 | INFO |   lock_text: False
2024-03-27,13:44:51 | INFO |   lock_text_freeze_layer_norm: False
2024-03-27,13:44:51 | INFO |   lock_text_unlocked_layers: 0
2024-03-27,13:44:51 | INFO |   log_every_n_steps: 100
2024-03-27,13:44:51 | INFO |   log_level: 20
2024-03-27,13:44:51 | INFO |   log_local: False
2024-03-27,13:44:51 | INFO |   log_path: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4/out.log
2024-03-27,13:44:51 | INFO |   logs: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/
2024-03-27,13:44:51 | INFO |   lr: 0.0005
2024-03-27,13:44:51 | INFO |   lr_cooldown_end: 0.0
2024-03-27,13:44:51 | INFO |   lr_cooldown_power: 1.0
2024-03-27,13:44:51 | INFO |   lr_scheduler: cosine
2024-03-27,13:44:51 | INFO |   model: ViT-B-16
2024-03-27,13:44:51 | INFO |   mrl_dim_to_consider: [768, 384, 192, 96, 48]
2024-03-27,13:44:51 | INFO |   mrl_loss_weights: [1.0, 1.0, 1.0, 1.0, 1.0]
2024-03-27,13:44:51 | INFO |   name: mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4
2024-03-27,13:44:51 | INFO |   no_set_device_rank: False
2024-03-27,13:44:51 | INFO |   precision: amp
2024-03-27,13:44:51 | INFO |   pretrained: 
2024-03-27,13:44:51 | INFO |   pretrained_image: False
2024-03-27,13:44:51 | INFO |   rank: 0
2024-03-27,13:44:51 | INFO |   remote_sync: None
2024-03-27,13:44:51 | INFO |   remote_sync_frequency: 300
2024-03-27,13:44:51 | INFO |   remote_sync_protocol: s3
2024-03-27,13:44:51 | INFO |   report_to: wandb
2024-03-27,13:44:51 | INFO |   resume: None
2024-03-27,13:44:51 | INFO |   save_frequency: 1
2024-03-27,13:44:51 | INFO |   save_most_recent: False
2024-03-27,13:44:51 | INFO |   seed: 42
2024-03-27,13:44:51 | INFO |   skip_scheduler: False
2024-03-27,13:44:51 | INFO |   tensorboard: False
2024-03-27,13:44:51 | INFO |   tensorboard_path: 
2024-03-27,13:44:51 | INFO |   torchscript: False
2024-03-27,13:44:51 | INFO |   trace: False
2024-03-27,13:44:51 | INFO |   train_data: /gscratch/krishna/chenhaoz/IL/FDT/data/cc3m/{00000..00331}.tar
2024-03-27,13:44:51 | INFO |   train_data_upsampling_factors: None
2024-03-27,13:44:51 | INFO |   train_num_samples: 3308333
2024-03-27,13:44:51 | INFO |   use_bn_sync: False
2024-03-27,13:44:51 | INFO |   val_data: None
2024-03-27,13:44:51 | INFO |   val_frequency: 1
2024-03-27,13:44:51 | INFO |   val_num_samples: None
2024-03-27,13:44:51 | INFO |   wandb: True
2024-03-27,13:44:51 | INFO |   wandb_notes: 
2024-03-27,13:44:51 | INFO |   wandb_project_name: mrl_clip_training
2024-03-27,13:44:51 | INFO |   warmup: 1000
2024-03-27,13:44:51 | INFO |   wd: 0.2
2024-03-27,13:44:51 | INFO |   workers: 4
2024-03-27,13:44:51 | INFO |   world_size: 4
2024-03-27,13:44:51 | INFO |   zeroshot_frequency: 1
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: Currently logged in as: arnabk1 (arnabk). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/wandb/run-20240327_134514-mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/arnabk/mrl_clip_training
wandb: üöÄ View run at https://wandb.ai/arnabk/mrl_clip_training/runs/mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4
wandb: WARNING Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
2024-03-27,13:45:53 | INFO | Start epoch 0
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[rank0]:[2024-03-27 13:47:47,328] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank1]:[2024-03-27 13:47:47,334] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank2]:[2024-03-27 13:47:47,341] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank3]:[2024-03-27 13:47:47,341] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
        return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,

  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
        exec(code, run_globals)exec(code, run_globals)

  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
    return _run_code(code, main_globals, None,
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
        main(sys.argv[1:])main(sys.argv[1:])

  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
        train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)

  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
    main(sys.argv[1:])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
    train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
Traceback (most recent call last):
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
    main(sys.argv[1:])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
    train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 94.25 MiB is free. Including non-PyTorch memory, this process has 44.24 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 3 has a total capacity of 44.35 GiB of which 146.25 MiB is free. Including non-PyTorch memory, this process has 44.19 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:[2024-03-27 13:47:52,657] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank3]:[2024-03-27 13:47:52,657] torch._dynamo.utils: [INFO] Function, Runtimes (s)
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 1 has a total capacity of 44.35 GiB of which 90.25 MiB is free. Including non-PyTorch memory, this process has 44.25 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]:[2024-03-27 13:47:52,658] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank1]:[2024-03-27 13:47:52,658] torch._dynamo.utils: [INFO] Function, Runtimes (s)
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 2 has a total capacity of 44.35 GiB of which 122.25 MiB is free. Including non-PyTorch memory, this process has 44.22 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]:[2024-03-27 13:47:52,659] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank2]:[2024-03-27 13:47:52,659] torch._dynamo.utils: [INFO] Function, Runtimes (s)
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.010 MB of 0.020 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb: üöÄ View run mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4 at: https://wandb.ai/arnabk/mrl_clip_training/runs/mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240327_134514-mrl_clip_cc3m_b256_accum_32_ep40_diffLogitScale_D032724_w1_gpu4_wkrs4/logs
[rank0]:[2024-03-27 13:47:59,212] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank0]:[2024-03-27 13:47:59,212] torch._dynamo.utils: [INFO] Function, Runtimes (s)
[2024-03-27 13:48:10,400] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 60783 closing signal SIGTERM
[2024-03-27 13:48:11,365] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 60784) of binary: /gscratch/krishna/arnabk1/pyclip/bin/python
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
training.main FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-27_13:48:10
  host      : g3076.hyak.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 60785)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-27_13:48:10
  host      : g3076.hyak.local
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 60786)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-27_13:48:10
  host      : g3076.hyak.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 60784)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
