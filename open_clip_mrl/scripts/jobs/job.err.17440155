[2024-03-29 09:55:42,315] torch.distributed.run: [WARNING] 
[2024-03-29 09:55:42,315] torch.distributed.run: [WARNING] *****************************************
[2024-03-29 09:55:42,315] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-29 09:55:42,315] torch.distributed.run: [WARNING] *****************************************
2024-03-29,09:59:36 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 4.
2024-03-29,09:59:36 | INFO | Loaded ViT-B-16 model config.
2024-03-29,09:59:36 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 4.
2024-03-29,09:59:36 | INFO | Loaded ViT-B-16 model config.
2024-03-29,09:59:36 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 4.
2024-03-29,09:59:36 | INFO | Loaded ViT-B-16 model config.
2024-03-29,09:59:36 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 4.
2024-03-29,09:59:36 | INFO | Loaded ViT-B-16 model config.
2024-03-29,09:59:41 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,09:59:42 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,09:59:42 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,09:59:42 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,09:59:44 | INFO | Model:
2024-03-29,09:59:44 | INFO | CLIP(
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (logit_scale): ParameterList(
      (0): Parameter containing: [torch.float32 of size  (cuda:0)]
      (1): Parameter containing: [torch.float32 of size  (cuda:0)]
      (2): Parameter containing: [torch.float32 of size  (cuda:0)]
      (3): Parameter containing: [torch.float32 of size  (cuda:0)]
      (4): Parameter containing: [torch.float32 of size  (cuda:0)]
  )
)
2024-03-29,09:59:44 | INFO | Params:
2024-03-29,09:59:44 | INFO |   accum_freq: 32
2024-03-29,09:59:44 | INFO |   aug_cfg: {}
2024-03-29,09:59:44 | INFO |   batch_size: 256
2024-03-29,09:59:44 | INFO |   beta1: 0.9
2024-03-29,09:59:44 | INFO |   beta2: 0.98
2024-03-29,09:59:44 | INFO |   checkpoint_path: /gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_/checkpoints
2024-03-29,09:59:44 | INFO |   coca_caption_loss_weight: 2.0
2024-03-29,09:59:44 | INFO |   coca_contrastive_loss_weight: 1.0
2024-03-29,09:59:44 | INFO |   copy_codebase: False
2024-03-29,09:59:44 | INFO |   csv_caption_key: title
2024-03-29,09:59:44 | INFO |   csv_img_key: filepath
2024-03-29,09:59:44 | INFO |   csv_separator: 	
2024-03-29,09:59:44 | INFO |   dataset_resampled: False
2024-03-29,09:59:44 | INFO |   dataset_type: webdataset
2024-03-29,09:59:44 | INFO |   ddp_static_graph: False
2024-03-29,09:59:44 | INFO |   debug: False
2024-03-29,09:59:44 | INFO |   delete_previous_checkpoint: False
2024-03-29,09:59:44 | INFO |   device: cuda:0
2024-03-29,09:59:44 | INFO |   dist_backend: nccl
2024-03-29,09:59:44 | INFO |   dist_url: env://
2024-03-29,09:59:44 | INFO |   distill: False
2024-03-29,09:59:44 | INFO |   distill_model: None
2024-03-29,09:59:44 | INFO |   distill_pretrained: None
2024-03-29,09:59:44 | INFO |   distributed: True
2024-03-29,09:59:44 | INFO |   epochs: 10
2024-03-29,09:59:44 | INFO |   epochs_cooldown: None
2024-03-29,09:59:44 | INFO |   eps: 1e-06
2024-03-29,09:59:44 | INFO |   force_custom_text: False
2024-03-29,09:59:44 | INFO |   force_image_size: None
2024-03-29,09:59:44 | INFO |   force_mrl_loss: True
2024-03-29,09:59:44 | INFO |   force_patch_dropout: None
2024-03-29,09:59:44 | INFO |   force_quick_gelu: False
2024-03-29,09:59:44 | INFO |   gather_with_grad: True
2024-03-29,09:59:44 | INFO |   grad_checkpointing: False
2024-03-29,09:59:44 | INFO |   grad_clip_norm: None
2024-03-29,09:59:44 | INFO |   horovod: False
2024-03-29,09:59:44 | INFO |   image_mean: None
2024-03-29,09:59:44 | INFO |   image_std: None
2024-03-29,09:59:44 | INFO |   imagenet_v2: None
2024-03-29,09:59:44 | INFO |   imagenet_val: /gscratch/krishna/arnabk1/root/val/
2024-03-29,09:59:44 | INFO |   local_loss: True
2024-03-29,09:59:44 | INFO |   local_rank: 0
2024-03-29,09:59:44 | INFO |   lock_image: False
2024-03-29,09:59:44 | INFO |   lock_image_freeze_bn_stats: False
2024-03-29,09:59:44 | INFO |   lock_image_unlocked_groups: 0
2024-03-29,09:59:44 | INFO |   lock_text: False
2024-03-29,09:59:44 | INFO |   lock_text_freeze_layer_norm: False
2024-03-29,09:59:44 | INFO |   lock_text_unlocked_layers: 0
2024-03-29,09:59:44 | INFO |   log_every_n_steps: 100
2024-03-29,09:59:44 | INFO |   log_level: 20
2024-03-29,09:59:44 | INFO |   log_local: False
2024-03-29,09:59:44 | INFO |   log_path: /gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_/out.log
2024-03-29,09:59:44 | INFO |   logs: /gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/
2024-03-29,09:59:44 | INFO |   lr: 1e-07
2024-03-29,09:59:44 | INFO |   lr_cooldown_end: 0.0
2024-03-29,09:59:44 | INFO |   lr_cooldown_power: 1.0
2024-03-29,09:59:44 | INFO |   lr_scheduler: cosine
2024-03-29,09:59:44 | INFO |   model: ViT-B-16
2024-03-29,09:59:44 | INFO |   mrl_dim_to_consider: [768, 384, 192, 96, 48]
2024-03-29,09:59:44 | INFO |   mrl_loss_weights: [1.0, 1.0, 1.0, 1.0, 1.0]
2024-03-29,09:59:44 | INFO |   name: ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_
2024-03-29,09:59:44 | INFO |   no_set_device_rank: False
2024-03-29,09:59:44 | INFO |   precision: amp
2024-03-29,09:59:44 | INFO |   pretrained: laion400m_e32
2024-03-29,09:59:44 | INFO |   pretrained_image: False
2024-03-29,09:59:44 | INFO |   rank: 0
2024-03-29,09:59:44 | INFO |   remote_sync: None
2024-03-29,09:59:44 | INFO |   remote_sync_frequency: 300
2024-03-29,09:59:44 | INFO |   remote_sync_protocol: s3
2024-03-29,09:59:44 | INFO |   report_to: wandb
2024-03-29,09:59:44 | INFO |   resume: None
2024-03-29,09:59:44 | INFO |   save_frequency: 1
2024-03-29,09:59:44 | INFO |   save_most_recent: False
2024-03-29,09:59:44 | INFO |   seed: 42
2024-03-29,09:59:44 | INFO |   skip_scheduler: False
2024-03-29,09:59:44 | INFO |   tensorboard: False
2024-03-29,09:59:44 | INFO |   tensorboard_path: 
2024-03-29,09:59:44 | INFO |   torchscript: False
2024-03-29,09:59:44 | INFO |   trace: False
2024-03-29,09:59:44 | INFO |   train_data: /gscratch/krishna/chenhaoz/IL/FDT/data/cc3m/{00000..00331}.tar
2024-03-29,09:59:44 | INFO |   train_data_upsampling_factors: None
2024-03-29,09:59:44 | INFO |   train_num_samples: 3308333
2024-03-29,09:59:44 | INFO |   use_bn_sync: False
2024-03-29,09:59:44 | INFO |   val_data: None
2024-03-29,09:59:44 | INFO |   val_frequency: 1
2024-03-29,09:59:44 | INFO |   val_num_samples: None
2024-03-29,09:59:44 | INFO |   wandb: True
2024-03-29,09:59:44 | INFO |   wandb_notes: 
2024-03-29,09:59:44 | INFO |   wandb_project_name: mrl_clip_training
2024-03-29,09:59:44 | INFO |   warmup: 500
2024-03-29,09:59:44 | INFO |   wd: 0.2
2024-03-29,09:59:44 | INFO |   workers: 4
2024-03-29,09:59:44 | INFO |   world_size: 4
2024-03-29,09:59:44 | INFO |   zeroshot_frequency: 1
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: Currently logged in as: arnabk1 (arnabk). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/wandb/run-20240329_095955-ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_
wandb: ⭐️ View project at https://wandb.ai/arnabk/mrl_clip_training
wandb: 🚀 View run at https://wandb.ai/arnabk/mrl_clip_training/runs/ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_
wandb: WARNING Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
2024-03-29,10:00:33 | INFO | Start epoch 0
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[rank0]:[2024-03-29 10:02:12,903] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank1]:[2024-03-29 10:02:12,936] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank2]:[2024-03-29 10:02:12,943] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank3]:[2024-03-29 10:02:12,953] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
    main(sys.argv[1:])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
    train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
    main(sys.argv[1:])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
    train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
    main(sys.argv[1:])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
    train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
    main(sys.argv[1:])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 400, in main
    train_one_epoch(model, data, loss, epoch, optimizer, scaler, scheduler, dist_model, args, tb_writer=writer)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/train.py", line 146, in train_one_epoch
    losses = loss(**inputs, logit_scale=logit_scale, output_dict=True)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 244, in forward
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 146.25 MiB is free. Including non-PyTorch memory, this process has 44.19 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    loss = super().forward(image_features=img, text_features=text, logit_scale=logit_scale[idx])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/open_clip/loss.py", line 128, in forward
    F.cross_entropy(logits_per_text, labels)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 1 has a total capacity of 44.35 GiB of which 170.25 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]:[2024-03-29 10:02:16,372] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank1]:[2024-03-29 10:02:16,372] torch._dynamo.utils: [INFO] Function, Runtimes (s)
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 2 has a total capacity of 44.35 GiB of which 594.25 MiB is free. Including non-PyTorch memory, this process has 43.76 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]:[2024-03-29 10:02:16,373] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank2]:[2024-03-29 10:02:16,373] torch._dynamo.utils: [INFO] Function, Runtimes (s)
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 3 has a total capacity of 44.35 GiB of which 128.25 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:[2024-03-29 10:02:16,374] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank3]:[2024-03-29 10:02:16,374] torch._dynamo.utils: [INFO] Function, Runtimes (s)
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb: 🚀 View run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_ at: https://wandb.ai/arnabk/mrl_clip_training/runs/ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240329_095955-ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07_/logs
[rank0]:[2024-03-29 10:02:21,780] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank0]:[2024-03-29 10:02:21,780] torch._dynamo.utils: [INFO] Function, Runtimes (s)
[2024-03-29 10:02:27,877] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 41909 closing signal SIGTERM
[2024-03-29 10:02:27,877] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 41910 closing signal SIGTERM
[2024-03-29 10:02:27,877] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 41912 closing signal SIGTERM
[2024-03-29 10:02:32,278] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 41911) of binary: /gscratch/krishna/arnabk1/pyclip/bin/python
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
training.main FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-29_10:02:27
  host      : g3065.hyak.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 41911)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
