[2024-03-29 07:49:50,109] torch.distributed.run: [WARNING] 
[2024-03-29 07:49:50,109] torch.distributed.run: [WARNING] *****************************************
[2024-03-29 07:49:50,109] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-29 07:49:50,109] torch.distributed.run: [WARNING] *****************************************
2024-03-29,07:52:46 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 4.
2024-03-29,07:52:46 | INFO | Loaded ViT-B-16 model config.
2024-03-29,07:52:46 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 4.
2024-03-29,07:52:46 | INFO | Loaded ViT-B-16 model config.
2024-03-29,07:52:46 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 4.
2024-03-29,07:52:46 | INFO | Loaded ViT-B-16 model config.
2024-03-29,07:52:47 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 4.
2024-03-29,07:52:47 | INFO | Loaded ViT-B-16 model config.
2024-03-29,07:52:51 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,07:52:51 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,07:52:52 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,07:52:52 | INFO | Loading pretrained ViT-B-16 weights (laion400m_e32).
2024-03-29,07:52:55 | INFO | Model:
2024-03-29,07:52:55 | INFO | CLIP(
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (logit_scale): ParameterList(
      (0): Parameter containing: [torch.float32 of size  (cuda:0)]
      (1): Parameter containing: [torch.float32 of size  (cuda:0)]
      (2): Parameter containing: [torch.float32 of size  (cuda:0)]
      (3): Parameter containing: [torch.float32 of size  (cuda:0)]
      (4): Parameter containing: [torch.float32 of size  (cuda:0)]
  )
)
2024-03-29,07:52:55 | INFO | Params:
2024-03-29,07:52:55 | INFO |   accum_freq: 32
2024-03-29,07:52:55 | INFO |   aug_cfg: {}
2024-03-29,07:52:55 | INFO |   batch_size: 256
2024-03-29,07:52:55 | INFO |   beta1: 0.9
2024-03-29,07:52:55 | INFO |   beta2: 0.98
2024-03-29,07:52:55 | INFO |   checkpoint_path: /gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07/checkpoints
2024-03-29,07:52:55 | INFO |   coca_caption_loss_weight: 2.0
2024-03-29,07:52:55 | INFO |   coca_contrastive_loss_weight: 1.0
2024-03-29,07:52:55 | INFO |   copy_codebase: False
2024-03-29,07:52:55 | INFO |   csv_caption_key: title
2024-03-29,07:52:55 | INFO |   csv_img_key: filepath
2024-03-29,07:52:55 | INFO |   csv_separator: 	
2024-03-29,07:52:55 | INFO |   dataset_resampled: False
2024-03-29,07:52:55 | INFO |   dataset_type: webdataset
2024-03-29,07:52:55 | INFO |   ddp_static_graph: False
2024-03-29,07:52:55 | INFO |   debug: False
2024-03-29,07:52:55 | INFO |   delete_previous_checkpoint: False
2024-03-29,07:52:55 | INFO |   device: cuda:0
2024-03-29,07:52:55 | INFO |   dist_backend: nccl
2024-03-29,07:52:55 | INFO |   dist_url: env://
2024-03-29,07:52:55 | INFO |   distill: False
2024-03-29,07:52:55 | INFO |   distill_model: None
2024-03-29,07:52:55 | INFO |   distill_pretrained: None
2024-03-29,07:52:55 | INFO |   distributed: True
2024-03-29,07:52:55 | INFO |   epochs: 10
2024-03-29,07:52:55 | INFO |   epochs_cooldown: None
2024-03-29,07:52:55 | INFO |   eps: 1e-06
2024-03-29,07:52:55 | INFO |   force_custom_text: False
2024-03-29,07:52:55 | INFO |   force_image_size: None
2024-03-29,07:52:55 | INFO |   force_mrl_loss: True
2024-03-29,07:52:55 | INFO |   force_patch_dropout: None
2024-03-29,07:52:55 | INFO |   force_quick_gelu: False
2024-03-29,07:52:55 | INFO |   gather_with_grad: True
2024-03-29,07:52:55 | INFO |   grad_checkpointing: False
2024-03-29,07:52:55 | INFO |   grad_clip_norm: None
2024-03-29,07:52:55 | INFO |   horovod: False
2024-03-29,07:52:55 | INFO |   image_mean: None
2024-03-29,07:52:55 | INFO |   image_std: None
2024-03-29,07:52:55 | INFO |   imagenet_v2: None
2024-03-29,07:52:55 | INFO |   imagenet_val: /gscratch/krishna/arnabk1/root/val/
2024-03-29,07:52:55 | INFO |   local_loss: True
2024-03-29,07:52:55 | INFO |   local_rank: 0
2024-03-29,07:52:55 | INFO |   lock_image: False
2024-03-29,07:52:55 | INFO |   lock_image_freeze_bn_stats: False
2024-03-29,07:52:55 | INFO |   lock_image_unlocked_groups: 0
2024-03-29,07:52:55 | INFO |   lock_text: False
2024-03-29,07:52:55 | INFO |   lock_text_freeze_layer_norm: False
2024-03-29,07:52:55 | INFO |   lock_text_unlocked_layers: 0
2024-03-29,07:52:55 | INFO |   log_every_n_steps: 100
2024-03-29,07:52:55 | INFO |   log_level: 20
2024-03-29,07:52:55 | INFO |   log_local: False
2024-03-29,07:52:55 | INFO |   log_path: /gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07/out.log
2024-03-29,07:52:55 | INFO |   logs: /gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/
2024-03-29,07:52:55 | INFO |   lr: 1e-07
2024-03-29,07:52:55 | INFO |   lr_cooldown_end: 0.0
2024-03-29,07:52:55 | INFO |   lr_cooldown_power: 1.0
2024-03-29,07:52:55 | INFO |   lr_scheduler: cosine
2024-03-29,07:52:55 | INFO |   model: ViT-B-16
2024-03-29,07:52:55 | INFO |   mrl_dim_to_consider: [768, 384, 192, 96, 48]
2024-03-29,07:52:55 | INFO |   mrl_loss_weights: [1.0, 1.0, 1.0, 1.0, 1.0]
2024-03-29,07:52:55 | INFO |   name: ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07
2024-03-29,07:52:55 | INFO |   no_set_device_rank: False
2024-03-29,07:52:55 | INFO |   precision: amp
2024-03-29,07:52:55 | INFO |   pretrained: laion400m_e32
2024-03-29,07:52:55 | INFO |   pretrained_image: False
2024-03-29,07:52:55 | INFO |   rank: 0
2024-03-29,07:52:55 | INFO |   remote_sync: None
2024-03-29,07:52:55 | INFO |   remote_sync_frequency: 300
2024-03-29,07:52:55 | INFO |   remote_sync_protocol: s3
2024-03-29,07:52:55 | INFO |   report_to: wandb
2024-03-29,07:52:55 | INFO |   resume: None
2024-03-29,07:52:55 | INFO |   save_frequency: 1
2024-03-29,07:52:55 | INFO |   save_most_recent: False
2024-03-29,07:52:55 | INFO |   seed: 42
2024-03-29,07:52:55 | INFO |   skip_scheduler: False
2024-03-29,07:52:55 | INFO |   tensorboard: False
2024-03-29,07:52:55 | INFO |   tensorboard_path: 
2024-03-29,07:52:55 | INFO |   torchscript: False
2024-03-29,07:52:55 | INFO |   trace: False
2024-03-29,07:52:55 | INFO |   train_data: /gscratch/krishna/chenhaoz/IL/FDT/data/cc3m/{00000..00331}.tar
2024-03-29,07:52:55 | INFO |   train_data_upsampling_factors: None
2024-03-29,07:52:55 | INFO |   train_num_samples: 3308333
2024-03-29,07:52:55 | INFO |   use_bn_sync: False
2024-03-29,07:52:55 | INFO |   val_data: None
2024-03-29,07:52:55 | INFO |   val_frequency: 1
2024-03-29,07:52:55 | INFO |   val_num_samples: None
2024-03-29,07:52:55 | INFO |   wandb: True
2024-03-29,07:52:55 | INFO |   wandb_notes: 
2024-03-29,07:52:55 | INFO |   wandb_project_name: mrl_clip_training
2024-03-29,07:52:55 | INFO |   warmup: 500
2024-03-29,07:52:55 | INFO |   wd: 0.2
2024-03-29,07:52:55 | INFO |   workers: 4
2024-03-29,07:52:55 | INFO |   world_size: 4
2024-03-29,07:52:55 | INFO |   zeroshot_frequency: 1
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: Currently logged in as: arnabk1 (arnabk). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: ERROR Error while calling W&B API: run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07 was previously created and deleted; try a new run name (<Response [409]>)
wandb: | Waiting for wandb.init()...wandb: ERROR Error while calling W&B API: run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07 was previously created and deleted; try a new run name (<Response [409]>)
wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: ERROR Error while calling W&B API: run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07 was previously created and deleted; try a new run name (<Response [409]>)
wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: ERROR Error while calling W&B API: run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07 was previously created and deleted; try a new run name (<Response [409]>)
wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: ERROR Error while calling W&B API: run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07 was previously created and deleted; try a new run name (<Response [409]>)
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: ERROR Error while calling W&B API: run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07 was previously created and deleted; try a new run name (<Response [409]>)
wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: ERROR Error while calling W&B API: run ViT-B-16_liaon400m_e32_b256_accum32_gpu4_finetune_mrl_ep10_warmup_500_lr1e-07 was previously created and deleted; try a new run name (<Response [409]>)
wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: ERROR Run initialization has timed out after 90.0 sec. 
wandb: ERROR Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 472, in <module>
    main(sys.argv[1:])
  File "/mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/training/main.py", line 376, in main
    wandb.init(
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1195, in init
    raise e
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1176, in init
    run = wi.init()
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 785, in init
    raise error
wandb.errors.CommError: Run initialization has timed out after 90.0 sec. 
Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
[rank0]:[2024-03-29 07:54:41,374] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[rank0]:[2024-03-29 07:54:41,374] torch._dynamo.utils: [INFO] Function, Runtimes (s)
[2024-03-29 07:54:50,467] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 73224 closing signal SIGTERM
[2024-03-29 07:54:50,467] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 73225 closing signal SIGTERM
[2024-03-29 07:54:50,467] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 73226 closing signal SIGTERM
[2024-03-29 07:54:51,320] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 73223) of binary: /gscratch/krishna/arnabk1/pyclip/bin/python
Traceback (most recent call last):
  File "/gscratch/krishna/arnabk1/pyclip/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
training.main FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-29_07:54:50
  host      : g3065.hyak.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 73223)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
