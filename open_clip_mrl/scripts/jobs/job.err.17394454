[2024-03-26 17:06:00,843] torch.distributed.run: [WARNING] 
[2024-03-26 17:06:00,843] torch.distributed.run: [WARNING] *****************************************
[2024-03-26 17:06:00,843] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-26 17:06:00,843] torch.distributed.run: [WARNING] *****************************************
2024-03-26,17:07:04 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 4.
2024-03-26,17:07:04 | INFO | Loaded ViT-B-16 model config.
2024-03-26,17:07:04 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 4.
2024-03-26,17:07:04 | INFO | Loaded ViT-B-16 model config.
2024-03-26,17:07:04 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 4.
2024-03-26,17:07:04 | INFO | Loaded ViT-B-16 model config.
2024-03-26,17:07:04 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 4.
2024-03-26,17:07:04 | INFO | Loaded ViT-B-16 model config.
2024-03-26,17:07:07 | INFO | Model:
2024-03-26,17:07:07 | INFO | CLIP(
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (logit_scale): ParameterList(
      (0): Parameter containing: [torch.float32 of size  (cuda:0)]
      (1): Parameter containing: [torch.float32 of size  (cuda:0)]
      (2): Parameter containing: [torch.float32 of size  (cuda:0)]
      (3): Parameter containing: [torch.float32 of size  (cuda:0)]
      (4): Parameter containing: [torch.float32 of size  (cuda:0)]
  )
)
2024-03-26,17:07:07 | INFO | Params:
2024-03-26,17:07:07 | INFO |   accum_freq: 16
2024-03-26,17:07:07 | INFO |   aug_cfg: {}
2024-03-26,17:07:07 | INFO |   batch_size: 256
2024-03-26,17:07:07 | INFO |   beta1: 0.9
2024-03-26,17:07:07 | INFO |   beta2: 0.98
2024-03-26,17:07:07 | INFO |   checkpoint_path: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/mrl_clip_cc12m_test_b256_accum_16_ep40_diffLogitScale_D032624_w1_gpu4_wkrs4/checkpoints
2024-03-26,17:07:07 | INFO |   coca_caption_loss_weight: 2.0
2024-03-26,17:07:07 | INFO |   coca_contrastive_loss_weight: 1.0
2024-03-26,17:07:07 | INFO |   copy_codebase: False
2024-03-26,17:07:07 | INFO |   csv_caption_key: title
2024-03-26,17:07:07 | INFO |   csv_img_key: filepath
2024-03-26,17:07:07 | INFO |   csv_separator: 	
2024-03-26,17:07:07 | INFO |   dataset_resampled: False
2024-03-26,17:07:07 | INFO |   dataset_type: webdataset
2024-03-26,17:07:07 | INFO |   ddp_static_graph: False
2024-03-26,17:07:07 | INFO |   debug: False
2024-03-26,17:07:07 | INFO |   delete_previous_checkpoint: False
2024-03-26,17:07:07 | INFO |   device: cuda:0
2024-03-26,17:07:07 | INFO |   dist_backend: nccl
2024-03-26,17:07:07 | INFO |   dist_url: env://
2024-03-26,17:07:07 | INFO |   distill: False
2024-03-26,17:07:07 | INFO |   distill_model: None
2024-03-26,17:07:07 | INFO |   distill_pretrained: None
2024-03-26,17:07:07 | INFO |   distributed: True
2024-03-26,17:07:07 | INFO |   epochs: 40
2024-03-26,17:07:07 | INFO |   epochs_cooldown: None
2024-03-26,17:07:07 | INFO |   eps: 1e-06
2024-03-26,17:07:07 | INFO |   force_custom_text: False
2024-03-26,17:07:07 | INFO |   force_image_size: None
2024-03-26,17:07:07 | INFO |   force_mrl_loss: True
2024-03-26,17:07:07 | INFO |   force_patch_dropout: None
2024-03-26,17:07:07 | INFO |   force_quick_gelu: False
2024-03-26,17:07:07 | INFO |   gather_with_grad: True
2024-03-26,17:07:07 | INFO |   grad_checkpointing: False
2024-03-26,17:07:07 | INFO |   grad_clip_norm: None
2024-03-26,17:07:07 | INFO |   horovod: False
2024-03-26,17:07:07 | INFO |   image_mean: None
2024-03-26,17:07:07 | INFO |   image_std: None
2024-03-26,17:07:07 | INFO |   imagenet_v2: None
2024-03-26,17:07:07 | INFO |   imagenet_val: /gscratch/krishna/arnabk1/root/val/
2024-03-26,17:07:07 | INFO |   local_loss: True
2024-03-26,17:07:07 | INFO |   local_rank: 0
2024-03-26,17:07:07 | INFO |   lock_image: False
2024-03-26,17:07:07 | INFO |   lock_image_freeze_bn_stats: False
2024-03-26,17:07:07 | INFO |   lock_image_unlocked_groups: 0
2024-03-26,17:07:07 | INFO |   lock_text: False
2024-03-26,17:07:07 | INFO |   lock_text_freeze_layer_norm: False
2024-03-26,17:07:07 | INFO |   lock_text_unlocked_layers: 0
2024-03-26,17:07:07 | INFO |   log_every_n_steps: 100
2024-03-26,17:07:07 | INFO |   log_level: 20
2024-03-26,17:07:07 | INFO |   log_local: False
2024-03-26,17:07:07 | INFO |   log_path: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/mrl_clip_cc12m_test_b256_accum_16_ep40_diffLogitScale_D032624_w1_gpu4_wkrs4/out.log
2024-03-26,17:07:07 | INFO |   logs: /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/scripts/logs/
2024-03-26,17:07:07 | INFO |   lr: 0.0005
2024-03-26,17:07:07 | INFO |   lr_cooldown_end: 0.0
2024-03-26,17:07:07 | INFO |   lr_cooldown_power: 1.0
2024-03-26,17:07:07 | INFO |   lr_scheduler: cosine
2024-03-26,17:07:07 | INFO |   model: ViT-B-16
2024-03-26,17:07:07 | INFO |   mrl_dim_to_consider: [768, 384, 192, 96, 48]
2024-03-26,17:07:07 | INFO |   mrl_loss_weights: [1.0, 1.0, 1.0, 1.0, 1.0]
2024-03-26,17:07:07 | INFO |   name: mrl_clip_cc12m_test_b256_accum_16_ep40_diffLogitScale_D032624_w1_gpu4_wkrs4
2024-03-26,17:07:07 | INFO |   no_set_device_rank: False
2024-03-26,17:07:07 | INFO |   precision: amp
2024-03-26,17:07:07 | INFO |   pretrained: 
2024-03-26,17:07:07 | INFO |   pretrained_image: False
2024-03-26,17:07:07 | INFO |   rank: 0
2024-03-26,17:07:07 | INFO |   remote_sync: None
2024-03-26,17:07:07 | INFO |   remote_sync_frequency: 300
2024-03-26,17:07:07 | INFO |   remote_sync_protocol: s3
2024-03-26,17:07:07 | INFO |   report_to: wandb
2024-03-26,17:07:07 | INFO |   resume: None
2024-03-26,17:07:07 | INFO |   save_frequency: 1
2024-03-26,17:07:07 | INFO |   save_most_recent: False
2024-03-26,17:07:07 | INFO |   seed: 42
2024-03-26,17:07:07 | INFO |   skip_scheduler: False
2024-03-26,17:07:07 | INFO |   tensorboard: False
2024-03-26,17:07:07 | INFO |   tensorboard_path: 
2024-03-26,17:07:07 | INFO |   torchscript: False
2024-03-26,17:07:07 | INFO |   trace: False
2024-03-26,17:07:07 | INFO |   train_data: /gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/{00000..00128}.tar
2024-03-26,17:07:07 | INFO |   train_data_upsampling_factors: None
2024-03-26,17:07:07 | INFO |   train_num_samples: 1280000
2024-03-26,17:07:07 | INFO |   use_bn_sync: False
2024-03-26,17:07:07 | INFO |   val_data: None
2024-03-26,17:07:07 | INFO |   val_frequency: 1
2024-03-26,17:07:07 | INFO |   val_num_samples: None
2024-03-26,17:07:07 | INFO |   wandb: True
2024-03-26,17:07:07 | INFO |   wandb_notes: 
2024-03-26,17:07:07 | INFO |   wandb_project_name: mrl_clip_training
2024-03-26,17:07:07 | INFO |   warmup: 4000
2024-03-26,17:07:07 | INFO |   wd: 0.2
2024-03-26,17:07:07 | INFO |   workers: 4
2024-03-26,17:07:07 | INFO |   world_size: 4
2024-03-26,17:07:07 | INFO |   zeroshot_frequency: 1
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: Currently logged in as: arnabk1 (arnabk). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /mmfs1/gscratch/krishna/arnabk1/mayank_clip_mrl/src/wandb/run-20240326_170718-mrl_clip_cc12m_test_b256_accum_16_ep40_diffLogitScale_D032624_w1_gpu4_wkrs4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mrl_clip_cc12m_test_b256_accum_16_ep40_diffLogitScale_D032624_w1_gpu4_wkrs4
wandb: ⭐️ View project at https://wandb.ai/arnabk/mrl_clip_training
wandb: 🚀 View run at https://wandb.ai/arnabk/mrl_clip_training/runs/mrl_clip_cc12m_test_b256_accum_16_ep40_diffLogitScale_D032624_w1_gpu4_wkrs4
wandb: WARNING Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
2024-03-26,17:07:50 | INFO | Start epoch 0
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/gscratch/krishna/arnabk1/pyclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[rank0]:[2024-03-26 17:08:41,216] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank1]:[2024-03-26 17:08:41,223] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank2]:[2024-03-26 17:08:41,302] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank3]:[2024-03-26 17:08:41,310] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
2024-03-26,17:08:59 | INFO | Train Epoch: 0 [  16384/1282048 (1%)] Data (t): 25.521 Batch (t): 68.760, 238.278/s, 59.5694/s/gpu LR: 0.000000 Logit Scale: 14.286 ,14.286 ,14.286 ,14.286 ,14.286 Mrl_clip_loss_768: 9.8163 (9.8163) Mrl_clip_loss_384: 9.8222 (9.8222) Mrl_clip_loss_192: 9.8897 (9.8897) Mrl_clip_loss_96: 9.9651 (9.9651) Mrl_clip_loss_48: 10.216 (10.216) Loss: 49.710 (49.710)
2024-03-26,17:09:46 | WARNING | Handling webdataset error (ReadError("unexpected end of data @ <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00086.tar'>")). Ignoring.
2024-03-26,17:09:46 | WARNING | Handling webdataset error (ReadError('unexpected end of data', <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00086.tar'>, '/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00086.tar')). Ignoring.
2024-03-26,17:13:50 | WARNING | Handling webdataset error (ReadError("unexpected end of data @ <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00003.tar'>")). Ignoring.
2024-03-26,17:13:50 | WARNING | Handling webdataset error (ReadError('unexpected end of data', <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00003.tar'>, '/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00003.tar')). Ignoring.
2024-03-26,17:18:18 | WARNING | Handling webdataset error (ReadError("unexpected end of data @ <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00012.tar'>")). Ignoring.
2024-03-26,17:18:18 | WARNING | Handling webdataset error (ReadError('unexpected end of data', <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00012.tar'>, '/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00012.tar')). Ignoring.
2024-03-26,17:25:56 | WARNING | Handling webdataset error (ReadError("unexpected end of data @ <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00099.tar'>")). Ignoring.
2024-03-26,17:25:56 | WARNING | Handling webdataset error (ReadError('unexpected end of data', <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00099.tar'>, '/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00099.tar')). Ignoring.
2024-03-26,17:41:09 | WARNING | Handling webdataset error (ReadError("unexpected end of data @ <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00087.tar'>")). Ignoring.
2024-03-26,17:41:09 | WARNING | Handling webdataset error (ReadError('unexpected end of data', <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00087.tar'>, '/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00087.tar')). Ignoring.
2024-03-26,17:47:32 | WARNING | Handling webdataset error (ReadError("unexpected end of data @ <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00086.tar'>")). Ignoring.
2024-03-26,17:47:32 | WARNING | Handling webdataset error (ReadError('unexpected end of data', <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00086.tar'>, '/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00086.tar')). Ignoring.
2024-03-26,17:49:27 | WARNING | Handling webdataset error (ReadError("unexpected end of data @ <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00003.tar'>")). Ignoring.
2024-03-26,17:49:27 | WARNING | Handling webdataset error (ReadError('unexpected end of data', <_io.BufferedReader name='/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00003.tar'>, '/gscratch/krishna/chenhaoz/IL/FDT/data/cc12m/00003.tar')). Ignoring.
slurmstepd: error: *** JOB 17394454 ON g3066 CANCELLED AT 2024-03-26T17:50:48 ***
